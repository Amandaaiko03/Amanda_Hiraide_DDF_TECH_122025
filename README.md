# ğŸš€ Case TÃ©cnico Dadosfera - Engenharia de Dados

**Candidata:** Amanda Aiko
**Data:** Dezembro/2025
**Objetivo:** ConstruÃ§Ã£o de uma Plataforma de Dados E2E (End-to-End) para E-commerce.

---

## ğŸ—„ï¸ Item 1 - SeleÃ§Ã£o da Base de Dados

* **Base Escolhida:** Brazilian E-Commerce Public Dataset by Olist
* **Fonte:** [Kaggle - Olist Dataset](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)

### Justificativa da Escolha
Para atender ao desafio de construir uma Plataforma de Dados robusta, selecionei o dataset da Olist. Esta escolha foi estratÃ©gica por trÃªs motivos principais:

1.  **Volumetria e Robustez:** ContÃ©m mais de **100.000 pedidos** (2016-2018), superando o requisito tÃ©cnico e permitindo anÃ¡lises temporais densas.
2.  **AderÃªncia Ã  Narrativa:** Cobre todos os desafios sugeridos:
    * *LogÃ­stica:* Dados de geolocalizaÃ§Ã£o, CEP e prazos (Estimado vs Real).
    * *Desempenho:* AvaliaÃ§Ã£o de vendedores e produtos.
    * *Dados Desestruturados:* Tabela de reviews com comentÃ¡rios reais para NLP/IA.
3.  **Estrutura Relacional:** Dados normalizados em 9 tabelas, ideais para demonstrar modelagem (Star Schema) e SQL complexo.

---

## âš™ï¸ Item 2.1 - IntegraÃ§Ã£o e IngestÃ£o de Dados

Realizei a ingestÃ£o dos dados na plataforma Dadosfera, focando na tabela fato principal (`olist_order_items`) e nas dimensÃµes satÃ©lites.

* **Status da Carga:** Sucesso âœ…
* **Volumetria:** 112.650 registros (Requisito > 100k atingido).

### EstratÃ©gia de Data Quality na Origem
Para garantir a integridade do schema (*Schema-on-Write*), identifiquei que a ingestÃ£o padrÃ£o de CSV poderia falhar na tipagem. Realizei um **prÃ©-processamento via Google Sheets**:
* FormataÃ§Ã£o nativa de `shipping_limit_date` para **Datetime**.
* FormataÃ§Ã£o de `price` e `freight_value` para **Decimal**.

Isso eliminou a necessidade de *casting* complexo e acelerou a disponibilidade dos dados na camada Silver.

**EvidÃªncia do Pipeline:**
![Pipeline de IngestÃ£o](assets/image_pipeline_sucesso.png)

---

## ğŸ“š Item 3 - ExploraÃ§Ã£o e GovernanÃ§a (Data Lake)

Organizei o Data Lake seguindo a **Arquitetura MedalhÃ£o (Medallion Architecture)** e os princÃ­pios FAIR (Findable, Accessible, Interoperable, Reusable).

### 1. Zonas de Dados
Classifiquei os ativos utilizando Tags na plataforma:

* ğŸ¥‰ **Camada Bronze (Landing Zone):** Dados brutos em formato original.
    * *Tags:* `LAKE:LANDING`, `BRONZE`.
* ğŸ¥ˆ **Camada Silver (Standardized):** Dados tipados e limpos.
    * *Tag Planejada:* `LAKE:SILVER`.
* ğŸ¥‡ **Camada Gold (Curated):** Dados modelados (Star Schema) para BI.
    * *Tag Planejada:* `LAKE:GOLD`.

### 2. DicionÃ¡rio de Dados
Cataloguei manualmente os ativos na Dadosfera:

* **`PUBLIC.OLIST_ORDER_ITEMS_DATASET` (Fato):** DocumentaÃ§Ã£o de colunas financeiras e conversÃ£o de tipos.
* **`PUBLIC.OLIST_PRODUCTS_DATASET` (DimensÃ£o):** Mapeamento de categorias e dados logÃ­sticos (peso/medidas).
* **`PUBLIC.OLIST_CUSTOMERS_DATASET` (DimensÃ£o):** IdentificaÃ§Ã£o de chaves geogrÃ¡ficas para anÃ¡lise espacial.
* **`PUBLIC.OLIST_ORDER_REVIEWS_DATASET` (Desestruturado):** IdentificaÃ§Ã£o de texto livre para GenAI.

---

## âœ… Item 4 - Data Quality & CDM

**EstratÃ©gia:** Utilizei a biblioteca **Great Expectations** para validar a camada Bronze antes da promoÃ§Ã£o para Silver, garantindo confiabilidade para os modelos de IA.

### Regras Implementadas (Expectation Suite)
* **Completeness:** `order_id` e `product_id` nÃ£o podem ser nulos.
* **Validity:** `price` e `freight_value` devem ser positivos (> 0).
* **Consistency:** `seller_id` deve ser do tipo string.

**Resultado:** Sucesso na validaÃ§Ã£o (100%).

### EvidÃªncias TÃ©cnicas
* ğŸ“„ **Notebook:** [Ver CÃ³digo de ValidaÃ§Ã£o](notebooks/data
